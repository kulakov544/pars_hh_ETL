# Анализ вакансий на hh.ru

### Бизнес требование
Нужно узнать какие навыки требуются на рынке вакансий.

### Техническое задание
Получить аналитику по требованиям к соискателям вакансий на сайте hh.ru по IT направлению.

### Решение
1. Построить базу DWH на PostgreSQL.
2. Получить данные о вакансиях с сайта hh.ru
3. Провести первичную обработка данных для загрузки в базу.
4. Загрузить данные в Stage слой DWH.
5. Трансформировать данные для укладки на долговременное хранение.
6. Заполнить справочники и таблицу вакансий в Core слое DWH, с сохранением обновленных записей в таблицу истории.
7. Оркестрация процесса через Prefect.
8. Сделать View и материализацию таблиц в DataMart слое DWH
9. По данным из DataMart построить дашборды в Yandex Data Lens и Power BI


### План реализации
Схема работы программы:
![Схема работы программы](flow_pars_hh_dir/file/DWH.png)
Подробную схему DWH можно посмотреть в соответствующем проекте:  
https://github.com/kulakov544/pars_hh_DWH.git

### Работа программы:
1. Создание списка наборов параметров search_params_list.
2. Создание сета для отбора уникальных id вакансий uniq_id
3. Разбивка наборов параметров на пакеты по 100 штук.
4. Формирование списка id вакансий.(vacancies_id_df)
   1. Получает 100 наборов параметров
   2. В цикле запрашивает данные по каждому набору параметров
   3. Получает вакансии по этому набору и проверяет есть ли id вакансий
   в uniq_id. Если нет то добавляет его туда и в массив vacancies_data.
   4. Когда пройдет по всем вакансиям из этого набора параметров добавляет их
   в all_vacancies_id_df и берет следующий набор параметров.
   5. Полученный датафрейм возвращается в vacancies_id_df
5.  По полученному списку id начинает собирать информацию о вакансиях.
      1. Запросы делаются отдельно для каждого id.
      2. В all_vacancies_data собирается основная информация о вакансии. 
      В all_vacancies_skill собираются скилы необходимые для этой вакансии.
    3. Тут же проходит первичная обработка. Проверки на наличие данных, добавление 
    хеша вакансии, изменение форматов дат...
    4. Полученные датафреймы возвращаются в vacancies_data_df и vacancies_skill_df
6. Если данные были собраны они загружаются в соответствующие таблицы с помощью put_data.
Это происходит с помощью replace по этому при каждой загрузке таблицы очищаются.
7. Когда данные загружены запускаются функции переноса данных из stage в core c
помощью функции update_core
   1. В таблицу stage_pars_hh добавляется столбец статуса. Он зависит от того есть 
   эта вакансия в базе и совпадают ли хеши с теми которые уже есть.
   2. Заполняются справочники включая скилы, если для них есть новые данные.
   3. Переносятся вакансии которых ещё не было в базе.
   4. Вакансии которые обновились, переносятся в fact_history и удаляются из основной таблицы.
   5. Вместо них загружаются новые записи.
   6. Обновляется таблица мапинга скилов и вакансий.
8. Всё это является потоком Prefect и в основных функциях проставлены таски.
9. Отдельным потоком является получение курсов валют.
10. В слое Data Mart созданы представления, которые строят таблицы с нужными данными.
По этим таблицам построен дашборд в PowerBI. Так же эти таблицы можно просто просмотреть любой 
программой для работы с базами данных.


## Используемые технологии

![Python](https://img.shields.io/badge/Python-3.12+-blue?logo=python)
![Pandas](https://img.shields.io/badge/Pandas-1.3.3+-yellow?logo=pandas)
![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-1.4.22+-orange?logo=python)
![Prefect](https://img.shields.io/badge/prefect-2.19.8+-green?logo=prefect)
![Requests](https://img.shields.io/badge/Requests-2.32.3+-green?logo=python)
![numpy](https://img.shields.io/badge/numpy-2.0.1+-green?logo=python)

## Установка

1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/kulakov544/prefect_dev.git 
   ```
2. Создайте виртуальное окружение и активируйте его:
   ```bash
   python -m venv venv
   source venv/bin/activate  # Для Windows используйте `venv\Scripts\activate`
   ```
3. Установите необходимые зависимости:
   ```bash
   pip install -r requirements.txt
   ```
4. В папке проекта создайте файл .env с данными для подключения к вашей базе данных.
access_token можно получить с помощью скриптов Authorization.py и authorization_code.py
если у вас есть коды для его получения, которые можно запросить на https://dev.hh.ru/
   ```bash
   conn_string = 'postgresql+psycopg2://user:password@localhost:port/mydatabase'
   access_token = "APP*"
   ```
5. Авторизуйтесь на сервере prefect
```
prefect cloud login
```
5. Запустите программу.
   ```bash
   python main.py
   ```
